train_dataset: cifar-fs
train_dataset_args: {split: train, augment: crop}
test_fs_dataset: cifar-fs
test_fs_dataset_args: {split: test}

# frequency of fs test during training. 100 means only testing at 100th epoch.
test_eval_fs_epoch: 100  
eval_fs_args: {n_way: 5, n_shots: [1, 5], n_query: 15, n_task: 200}
# val_fs_dataset: cifar-fs
# val_fs_dataset_args: {split: val}
# val_eval_fs_epoch: 100

model: classifier
model_args:
    encoder: resnet12
    encoder_args: {}
    classifier: nn-classifier 
    classifier_args: {n_classes: 64}

batch_size: 128
max_epoch: 100
optimizer: sgd
optimizer_args: {lr: 0.05, weight_decay: 5.e-4, milestones: [60,80]}
fs_iters: 5 # fs test iters

# 'load' is used for recovering a breakpoint. Just add the .pth file path you want to recover in this yaml and rerun the python file, the training procedure will continue at the breakpoint.
# load: ./save/rn12/cifar_als_flooding/epoch-last.pth

save_epoch: 100  # frequency of saving checkpoints during training. 100 means only saving the 100th epoch.

# the PGD params in AT
adversary: 
    eps: 8      # perturbations range
    alpha: 2    # perturbation step
    iters: 7    # iteration in PGD
    norm: inf

# ALS params, need to be commented when trained with vanilla AT
# als:
#     gamma: 0.1
#     eta: 0.9
#     minval: 0     # no need to change
#     maxval: 1     # no need to change
#     inner: True   # When set to True, PGD generation also adopts ALS loss

# PGD params in fs test
fs_attack:
    eps: 8
    alpha: 2
    iters: 20
    norm: inf

# PGD params in validation set (which is unnecessary for CIFAR-FS)
# val_attack: 
#     eps: 8
#     alpha: 2
#     iters: 7
#     norm: inf